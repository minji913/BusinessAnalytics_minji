{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트 마이닝\n",
    "\n",
    "일정한 길이의 vector로 변환\n",
    "\n",
    "변환된 vector에 머신러닝/딥러닝 기법을 적용\n",
    "\n",
    "##텍스트 마이닝 적용분야\n",
    "\n",
    "document classification\n",
    "\n",
    "document generation\n",
    "\n",
    "keyword extraction\n",
    "\n",
    "topic modeling\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트마이닝 도구- 파이썬\n",
    "\n",
    "NLTK:가장 많이 알려진 NLP 라이브러리\n",
    "\n",
    "Scikit Learn: 머신러닝 라이브러리\n",
    "\n",
    "Gensim: Word2Vec으로 유명\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트 마이닝 기본도구\n",
    "\n",
    "목적: document, sentence 등을 sparse vector로 변환\n",
    "    \n",
    "Tokenize: 대상이 되는 문서/문장을 최소 단위로 쪼갬\n",
    "    \n",
    "Text normalization: 최소단위를 표준화\n",
    "    \n",
    "Pos-tagging: 최소 의미단위로 나누어진 대상에 대해 품사를 부착\n",
    "    \n",
    "Chunking: pos-tagging의 결과를 명사구, 형용사구, 분사구 등과 같은 말모듬으로 다시 합치는 과정\n",
    "    \n",
    "Bow,TFIDF: tokenized결과를 이용하여 문서를 vector로 표현    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'this', 'class', ',', 'we', 'use', 'the', 'am', 'actively', 'looking', 'for', 'Ph.D.', 'students', '.', 'and', 'you', 'are', 'a', 'Ph.D.', 'student', '.']\n"
     ]
    }
   ],
   "source": [
    "#Tokenize 예시\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "text=\"In this class, we use the  am actively looking for Ph.D. students. and you are a Ph.D. student.\"\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PorterStemmer>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#text normalization-stemming-porter stemmer(어간 추출) 예시\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. cooking : cook\n",
      "2. cookery : cookeri\n",
      "3. believes : believ\n",
      "4. using : use\n"
     ]
    }
   ],
   "source": [
    "print(\"1. cooking :\", ps.stem(\"cooking\"))\n",
    "print(\"2. cookery :\", ps.stem(\"cookery\"))\n",
    "print(\"3. believes :\", ps.stem(\"believes\"))\n",
    "print(\"4. using :\", ps.stem(\"using\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatization(표제어 추출)예시\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-1. cooking : cooking\n",
      "1-2. cooking(pos=\"v\") : cook\n",
      "1-3. cooking(pos=\"n\") : cooking\n",
      "2. cookbooks : cookbook\n",
      "3. believes : believe\n",
      "4. buses : bus\n",
      "5. using : use\n"
     ]
    }
   ],
   "source": [
    "print(\"1-1. cooking :\", lemmatizer.lemmatize(\"cooking\"))\n",
    "print(\"1-2. cooking(pos=\\\"v\\\") :\", lemmatizer.lemmatize(\"cooking\", pos=\"v\"))\n",
    "print(\"1-3. cooking(pos=\\\"n\\\") :\", lemmatizer.lemmatize(\"cooking\", pos=\"n\"))\n",
    "print(\"2. cookbooks :\", lemmatizer.lemmatize(\"cookbooks\"))\n",
    "print(\"3. believes :\", lemmatizer.lemmatize(\"believes\", pos=\"v\"))\n",
    "print(\"4. buses :\", lemmatizer.lemmatize(\"buses\"))\n",
    "print(\"5. using :\", lemmatizer.lemmatize(\"using\", pos=\"v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('little', 'JJ'),\n",
       " ('yellow', 'JJ'),\n",
       " ('dog', 'NN'),\n",
       " ('barked', 'VBD'),\n",
       " ('at', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('Persian', 'JJ'),\n",
       " ('cat', 'NN')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pos-tagging 예시\n",
    "tokens = \"The little yellow dog barked at the Persian cat\".split()\n",
    "tags_en = nltk.pos_tag(tokens)\n",
    "tags_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chunking 예시\n",
    "parser_en = nltk.RegexpParser(\"NP: {<DT>?<JJ>?<NN.*>*}\")\n",
    "chunks_en = parser_en.parse(tags_en)\n",
    "chunks_en.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 개체명 인식\n",
    "개체명은 기관, 단체, 사람, 날짜등과 같이 특정 정보에 해당하는 명사구를 의미\n",
    "\n",
    "관계인식(Relation Extraction) \n",
    "\n",
    "NER에 의해 추출된 개체명들을 대상으로 그들간의 관계를 추출 하는 작업 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOW(Bag of Words)\n",
    "\n",
    "Vector Space Model: 문서를 bag of words로 표현, 단어가 쓰여진 순서는 무시\n",
    "\n",
    "BOW에 있어서 중요한 것은 단어의 등장 빈도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF\n",
    "\n",
    "단어의 count를 단어가 나타난 문서의 수로 나눠서 자주 등장하지 않는 단어의 weight를 올림\n",
    "\n",
    "tf(d, t): 문서d에 단어t가 나타난 횟수, count vector와 동일, 로그스케일 등 다양한 변형이 있음\n",
    "    \n",
    "df(t): 전체 문서 중에서 단어t를 포함하는 문서의 수\n",
    "\n",
    "idf(t): df(t)의 역수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # 데이터프레임 사용을 위해\n",
    "from math import log # IDF 계산을 위해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "  '먹고 싶은 사과',\n",
    "  '먹고 싶은 바나나',\n",
    "  '길고 노란 바나나 바나나',\n",
    "  '저는 과일이 좋아요'\n",
    "] \n",
    "vocab = list(set(w for doc in docs for w in doc.split()))\n",
    "vocab.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(docs) # 총 문서의 수\n",
    "\n",
    "def tf(t, d):\n",
    "    return d.count(t)\n",
    "\n",
    "def idf(t):\n",
    "    df = 0\n",
    "    for doc in docs:\n",
    "        df += t in doc\n",
    "    return log(N/(df + 1))\n",
    "\n",
    "def tfidf(t, d):\n",
    "    return tf(t,d)* idf(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>과일이</th>\n",
       "      <th>길고</th>\n",
       "      <th>노란</th>\n",
       "      <th>먹고</th>\n",
       "      <th>바나나</th>\n",
       "      <th>사과</th>\n",
       "      <th>싶은</th>\n",
       "      <th>저는</th>\n",
       "      <th>좋아요</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   과일이  길고  노란  먹고  바나나  사과  싶은  저는  좋아요\n",
       "0    0   0   0   1    0   1   1   0    0\n",
       "1    0   0   0   1    1   0   1   0    0\n",
       "2    0   1   1   0    2   0   0   0    0\n",
       "3    1   0   0   0    0   0   0   1    1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = []\n",
    "for i in range(N): # 각 문서에 대해서 아래 명령을 수행\n",
    "    result.append([])\n",
    "    d = docs[i]\n",
    "    for j in range(len(vocab)):\n",
    "        t = vocab[j]        \n",
    "        result[-1].append(tf(t, d))\n",
    "\n",
    "tf_ = pd.DataFrame(result, columns = vocab)\n",
    "tf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>과일이</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>길고</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>노란</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>먹고</th>\n",
       "      <td>0.287682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>바나나</th>\n",
       "      <td>0.287682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>사과</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>싶은</th>\n",
       "      <td>0.287682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>저는</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>좋아요</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          IDF\n",
       "과일이  0.693147\n",
       "길고   0.693147\n",
       "노란   0.693147\n",
       "먹고   0.287682\n",
       "바나나  0.287682\n",
       "사과   0.693147\n",
       "싶은   0.287682\n",
       "저는   0.693147\n",
       "좋아요  0.693147"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = []\n",
    "for j in range(len(vocab)):\n",
    "    t = vocab[j]\n",
    "    result.append(idf(t))\n",
    "\n",
    "idf_ = pd.DataFrame(result, index = vocab, columns = [\"IDF\"])\n",
    "idf_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification with BOW/TFIDF\n",
    "Naive Bayes 나이브 베이즈\n",
    "\n",
    "Logistic regression 로지스틱 회귀\n",
    "\n",
    "Decision tree 의사결정 나무"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 한글 감성분석\n",
    "\n",
    "label이 0이면 부정, 1이면 긍정\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 주성분 분석\n",
    "\n",
    "선형결합\n",
    "\n",
    "공분산 행렬\n",
    "\n",
    "고유값이 큰 순서대로 고유벡터를 정렬하여 차원 선택\n",
    "\n",
    "선택된 고유벡터와 X의 선형결합으로 차원 축소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length</th>\n",
       "      <th>sepal width</th>\n",
       "      <th>petal length</th>\n",
       "      <th>petal width</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length  sepal width  petal length  petal width       target\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
       "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
       "4           5.0          3.6           1.4          0.2  Iris-setosa"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "df = pd.read_csv(url, names=['sepal length','sepal width','petal length','petal width','target'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length</th>\n",
       "      <th>sepal width</th>\n",
       "      <th>petal length</th>\n",
       "      <th>petal width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.900681</td>\n",
       "      <td>1.032057</td>\n",
       "      <td>-1.341272</td>\n",
       "      <td>-1.312977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.143017</td>\n",
       "      <td>-0.124958</td>\n",
       "      <td>-1.341272</td>\n",
       "      <td>-1.312977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.385353</td>\n",
       "      <td>0.337848</td>\n",
       "      <td>-1.398138</td>\n",
       "      <td>-1.312977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.506521</td>\n",
       "      <td>0.106445</td>\n",
       "      <td>-1.284407</td>\n",
       "      <td>-1.312977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.021849</td>\n",
       "      <td>1.263460</td>\n",
       "      <td>-1.341272</td>\n",
       "      <td>-1.312977</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length  sepal width  petal length  petal width\n",
       "0     -0.900681     1.032057     -1.341272    -1.312977\n",
       "1     -1.143017    -0.124958     -1.341272    -1.312977\n",
       "2     -1.385353     0.337848     -1.398138    -1.312977\n",
       "3     -1.506521     0.106445     -1.284407    -1.312977\n",
       "4     -1.021849     1.263460     -1.341272    -1.312977"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler  # 표준화 패키지 라이브러리 \n",
    "x = df.drop(['target'], axis=1).values # 독립변인들의 value값만 추출\n",
    "y = df['target'].values # 종속변인 추출\n",
    "\n",
    "x = StandardScaler().fit_transform(x) # x객체에 x를 표준화한 데이터를 저장\n",
    "\n",
    "features = ['sepal length', 'sepal width', 'petal length', 'petal width']\n",
    "pd.DataFrame(x, columns=features).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2) # 주성분을 몇개로 할지 결정\n",
    "printcipalComponents = pca.fit_transform(x)\n",
    "principalDf = pd.DataFrame(data=printcipalComponents, columns = ['principal component1', 'principal component2'])\n",
    "# 주성분으로 이루어진 데이터 프레임 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>principal component1</th>\n",
       "      <th>principal component2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.264542</td>\n",
       "      <td>0.505704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.086426</td>\n",
       "      <td>-0.655405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.367950</td>\n",
       "      <td>-0.318477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.304197</td>\n",
       "      <td>-0.575368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.388777</td>\n",
       "      <td>0.674767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   principal component1  principal component2\n",
       "0             -2.264542              0.505704\n",
       "1             -2.086426             -0.655405\n",
       "2             -2.367950             -0.318477\n",
       "3             -2.304197             -0.575368\n",
       "4             -2.388777              0.674767"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "principalDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.72770452, 0.23030523])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOPIC MODELING 토픽 모델링\n",
    "\n",
    "토픽모델링의 원리: 토픽은 주제를 의미하는 용어로 사용되며, 각 문서들이 특정한 주제에 속할 확률분포와 주제로부터 특정 단어들이 파생되어 나올 확률분호가 주어졌을 때, 이 두 확률분포를 조합하여 각 문서들에 들어가는 단어들의 확률분포를 계산\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-35-465c18de9b03>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-35-465c18de9b03>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    'talk.politics.mideast', 'soc.religion.christian', 'sci.electronics' , 'sci.me\u001b[0m\n\u001b[1;37m                                                                                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "# 주어진 데이터셋의 일부 카톄고리 데이터만 추출하므로 카테고리 사전에 설정 \n",
    "cats = ['rec.motorcycles', 'rec.sport.baseball' , \n",
    "'comp.graphics' , 'comp.windows.x' , \n",
    "'talk.politics.mideast', 'soc.religion.christian', 'sci.electronics' , 'sci.me \n",
    "'sci.med'] \n",
    "# 설정해준 카테고리의 데이터들만 추출 \n",
    "news_df = fetch_20newsgroups(subset='all' , remove=('headers','footers','quotes'),categories=cats,random_state=12)\n",
    "\n",
    "# CountVectorizer로 텍스트 데이터들 단어 빈도수에 기반해 벡터화시키기 \n",
    "\n",
    "count_vect= CountVectorizer(max_df=0.95, max_features=1000 , min_df=2, stop_words='english', ngram_range=(1,2))\n",
    "\n",
    "ftr_vect=count_vect.fit_transform(news_df.data)\n",
    "\n",
    "# LDA클래스를 이용해서 피처 벡터화시킨 것을 토픽모델링 시키기\n",
    "\n",
    "# 8개의 주제만 뽑았으니 n components(토픽개수) 8로설정\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=8, random_state=42)  \n",
    "\n",
    "Ida.fit(ftr_vect) \n",
    "# components 속성은 8걔의 토픽별(row)로 1000개의 feature(단어)들의 분포수치를 보여줌\n",
    "print (Ida.components_.shape) \n",
    "print (Ida.components_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding\n",
    "\n",
    "단어에 대한 vector의 dimension reduction이 목표\n",
    "\n",
    "one-hot-encoding\n",
    "\n",
    "word embedding: one-hot-encoding으로 표현된 단어를 dense vector로 변환\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['나', '는', '자연어', '처리', '를', '배운다']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt  \n",
    "okt=Okt()  \n",
    "token=okt.morphs(\"나는 자연어 처리를 배운다\")   #토큰화 수행\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'나': 0, '는': 1, '자연어': 2, '처리': 3, '를': 4, '배운다': 5}\n"
     ]
    }
   ],
   "source": [
    "word2index={} #각 토큰에 고유한 인덱스 부여\n",
    "for voca in token:\n",
    "     if voca not in word2index.keys():\n",
    "       word2index[voca]=len(word2index)\n",
    "print(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(word, word2index):\n",
    "       one_hot_vector = [0]*(len(word2index)) #토큰을 입력하면 해당 토큰에 대한 one-hot-vector를 만들어내는 함수\n",
    "       index=word2index[word]\n",
    "       one_hot_vector[index]=1\n",
    "       return one_hot_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 0, 0, 0]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoding(\"자연어\",word2index) #함수에 '자연어'라는 토큰을 입력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "\n",
    "문장에 나타난 단어들의 순서를 이용해 word embedding을 수행\n",
    "\n",
    "CBOW:주변 단어들을 이용하여 다음 단어를 예측\n",
    "\n",
    "Skip-gram: 한 단어의 주변단어들을 예측\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pmg09\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "sentences=[['first','second','third','fourth']]*1000 #각 단어를 문장 목록으로 설정\n",
    "\n",
    "model1 = gensim.models.Word2Vec(sentences, min_count = 1, size = 2) #2개의 동일한 모델구현\n",
    "model2 = gensim.models.Word2Vec(sentences, min_count = 1, size = 2)\n",
    "print(all(model1['first']==model2['first']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pmg09\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "model3 = gensim.models.Word2Vec(sentences, min_count = 1, size = 2, seed = 1234)\n",
    "model4 = gensim.models.Word2Vec(sentences, min_count = 1, size = 2, seed = 1234)\n",
    "print(all(model3['first']==model4['first']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RBM\n",
    "사전학습 목적으로 개발\n",
    "\n",
    "Deep NN의 vanishing gradient 문제 해결을 위해 제안\n",
    "\n",
    "#Autoencoder\n",
    "RBM과 유사한 개념"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# context의 파악\n",
    "\n",
    "N-gram: 문맥을 파악하기 위한 전통적인 방법(bi-gram,tri-gram)\n",
    "       \n",
    "대상이 되는 문자열을 하나의 단어 단위가 아닌 2개 이상의 단위로 잘라서 처리\n",
    "\n",
    "딥러닝-RNN: 문장을 단어들의 시퀀스 혹은 시리즈로 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "\n",
    "RNN의 문제: 문장이 길수록 층이 깊은 형태를 갖게됨-->앞부분의 단어 정보가 학습되지 않음\n",
    "\n",
    "LSTM: 직통 통로를 만들어 RNN의 문제를 해결\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN(합성곱 신경망)\n",
    "\n",
    "CNN은 원래 이미 처리를 위해 개발된 신경망으로, 현재는 인간의 이미지 인식보다 더 나은 인식 성능을 보이고 있음\n",
    "\n",
    "합성곱층과 풀링층으로 구성되며, 합성곱층은 2차원 이미지에서 특정 영역의 특징을 추출하는 역할을 한다. 이는 연속된 단어들의 특징을 추출하는 것과 유사한 특성이 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence to Sequence\n",
    "\n",
    "지금까지 입력은 sequence, 출력은 하나의 값인 경우가 일반적이었으나\n",
    "\n",
    "번역, 챗봇, summarize 등은 출력도 sequence가 되어야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이썬의 딕셔너리 자료형을 선언\n",
    "# 키(Key) : 값(value)의 형식으로 키와 값의 쌍(Pair)을 선언한다.\n",
    "dict = {\"2017\" : \"Transformer\", \"2018\" : \"BERT\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer\n"
     ]
    }
   ],
   "source": [
    "print(dict[\"2017\"]) #2017이라는 키에 해당되는 값을 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT\n"
     ]
    }
   ],
   "source": [
    "print(dict[\"2018\"])  #2018이라는 키에 해당되는 값을 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
